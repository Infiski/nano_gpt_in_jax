{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4MQc64FbeZtt"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ModelConfig:\n",
        "  vocab_size : int = 50257\n",
        "  n_head : int = 12\n",
        "  n_embed : int = 768\n",
        "  n_layer : int = 12\n",
        "  block_size : int = 1024\n",
        "  dropout_rate : float = 0.1\n"
      ],
      "metadata": {
        "id": "13lCGJksfDIy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp"
      ],
      "metadata": {
        "id": "vieAua-CXYvF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flax import linen as nn\n",
        "import jax.numpy as jnp\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "  config:ModelConfig\n",
        "\n",
        "  @nn.compact\n",
        "\n",
        "  def __call__(self, x, deterministic = True):\n",
        "    assert len(x.shape)==3\n",
        "    b, l, d = x.shape\n",
        "\n",
        "    q = nn.Dense(self.config.n_embed)(x)\n",
        "    k = nn.Dense(self.config.n_embed)(x)\n",
        "    v = nn.Dense(self.config.n_embed)(x)\n",
        "\n",
        "    q = jnp.reshape(q, (b, l, d//self.config.n_head, self.config.n_head))\n",
        "    k = jnp.reshape(k, (b, l, d//self.config.n_head, self.config.n_head))\n",
        "    v = jnp.reshape(v, (b, l, d//self.config.n_head, self.config.n_head))\n",
        "\n",
        "    norm = jnp.sqrt(list(jnp.shape(k))[-1])\n",
        "\n",
        "    attn = jnp.matmul(q, jnp.transpose(k, (0, 1, 3, 2))) / norm\n",
        "    mask = jnp.tril(attn)\n",
        "\n",
        "    attn = jnp.where(mask[:, :, :l, :l], attn, float(\"-inf\"))\n",
        "    probs = jax.nn.softmax(attn, axis=-1)\n",
        "    y = jnp.matmul(probs, v)\n",
        "    y = jnp.reshape(y, (b, l, d))\n",
        "    y = nn.Dense(self.config.n_embed)(y)\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YPLJw7g3fzzQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "  config : ModelConfig\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, deterministic = True):\n",
        "    x=nn.Dense(self.config.n_embed*4)(x)\n",
        "    x=nn.gelu(x)\n",
        "    x=nn.Dropout(rate=self.config.dropout_rate)(x, deterministic=True)\n",
        "    x=nn.Dense(self.config.n_embed)(x)\n",
        "    x=nn.Dropout(rate=self.config.dropout_rate)(x, deterministic=True)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "  config : ModelConfig\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    x = nn.LayerNorm()(x)\n",
        "    x = x + CausalSelfAttention(self.config)(x)\n",
        "    x = nn.LayerNorm()(x)\n",
        "    x = x + MLP(self.config)(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "kIxHxhzdnLQF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "  config : ModelConfig\n",
        "\n",
        "  @nn.compact\n",
        "\n",
        "  def __call__(self, x, deterministic=True):\n",
        "    B, T = x.shape\n",
        "    assert T <= self.config.block_size\n",
        "\n",
        "    pos = jnp.arange(0, T)[None]\n",
        "    pos_emb = nn.Embed(self.config.block_size, self.config.n_embed)(pos)\n",
        "    wte = nn.Embed(self.config.vocab_size, self.config.n_embed)\n",
        "    tok_emb = wte(x)\n",
        "    x = tok_emb + pos_emb\n",
        "\n",
        "    for _ in range(self.config.n_layer):\n",
        "      x = Block(self.config)(x)\n",
        "    x = nn.LayerNorm()(x)\n",
        "    logits = nn.Dense(config.n_embed, config.vocab_size)(x)\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n",
        "  def init(self, rng):\n",
        "    tokens = jnp.zeros((1, self.config.block_size), dtype=jnp.uint16)\n",
        "    params = jax.jit(super().init, static_argnums=(2,))(rng, tokens, True)\n",
        "    return params\n",
        "\n"
      ],
      "metadata": {
        "id": "Dbfc51LYnLNv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_params(params):\n",
        "  p=jax.tree_util.tree_map(lambda a: a.size if isinstance(a, jnp.ndarray) else 0, params)\n",
        "  return jax.tree_util.tree_reduce(lambda a, b : a+b, p)"
      ],
      "metadata": {
        "id": "AURmPVe8nLLF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = ModelConfig()\n",
        "key = jax.random.PRNGKey(0)\n",
        "model = GPT(config)\n",
        "params = model.init(key)\n",
        "# count_params(params)"
      ],
      "metadata": {
        "id": "xyho-hDRnLIE"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "8yO-7I3zv-Jb",
        "outputId": "54415877-68cd-44a5-ca78-5b141ab15c23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT(\n",
            "    # attributes\n",
            "    config = ModelConfig(vocab_size=50257, n_head=12, n_embed=768, n_layer=12, block_size=1024, dropout_rate=0.1)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4OxtyAWoiBSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoModelForCausalLM\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "# print(model)\n"
      ],
      "metadata": {
        "id": "bS-gt9ADs32_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyoYTh7Ns30T",
        "outputId": "c7d91144-09da-47e0-fc24-5e6d6ded733c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'params': {'Dense_0': {'kernel': Array([[-0.95366824,  0.43563786, -0.7954482 , -0.49190977],\n",
              "          [ 0.5308177 ,  0.74109775,  0.6027838 , -0.02684463],\n",
              "          [ 0.24410059,  0.44881055, -1.050442  ,  0.4932145 ]],      dtype=float32),\n",
              "   'bias': Array([0., 0., 0., 0.], dtype=float32)},\n",
              "  'Dense_1': {'kernel': Array([[ 0.00166371,  0.16012576],\n",
              "          [ 0.09040862, -0.42028674],\n",
              "          [ 0.32189375,  0.43688348],\n",
              "          [-0.5580085 , -0.36031362]], dtype=float32),\n",
              "   'bias': Array([0., 0.], dtype=float32)}}}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader:\n",
        "  def __init__(self, B, T):\n",
        "    self.current_position = 0\n",
        "    self.B = B\n",
        "    self.T = T\n",
        "\n",
        "    with open(\"/content/input.txt\",\"r\") as f:\n",
        "      text = f.read()\n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "    self.tokens = jnp.array(enc.encode(text))\n",
        "    print(f\"loaded {len(self.tokens)} tokens in the datasets\" )\n",
        "    print(f\" 1 epoch = {len(self.tokens)//(B*T)} batches\")\n",
        "\n",
        "  def next_batch(self):\n",
        "    B,T = self.B, self.T\n",
        "    buf = self.tokens[self.current_position:self.current_position+B*T+1]\n",
        "    x,y = jnp.reshape(buf[:-1],(B,T)), jnp.reshape(buf[1:],(B,T))\n",
        "    self.current_position += B*T\n",
        "    if self.current_position + B*T+1 > len(self.tokens):\n",
        "      self.current_position = 0\n",
        "    return x,y"
      ],
      "metadata": {
        "id": "g1M-SMxPtDzy"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "from flax import linen as nn\n",
        "from flax.training.train_state import TrainState  # <- THIS is the TrainState you need\n",
        "from flax.core import FrozenDict\n",
        "from typing import Tuple\n",
        "\n",
        "\n",
        "def init_train_state(key, config) -> TrainState:\n",
        "  model = GPT(config)\n",
        "  params = model.init(key)\n",
        "  optimizer = optax.adamw(3e-4, b1=0.9, b2=0.98, eps=1e-9, weight_decay=1e-1)\n",
        "  train_state = TrainState.create(\n",
        "        apply_fn=model.apply,\n",
        "        params=params,\n",
        "        tx=optimizer)\n",
        "  return train_state\n",
        "\n",
        "@jax.jit\n",
        "def train_step(state: TrainState, x: jnp.ndarray, y: jnp.ndarray) -> Tuple[jnp.ndarray, TrainState]:\n",
        "\n",
        "  def loss_fn(params: FrozenDict) -> jnp.ndarray:\n",
        "\n",
        "      logits = state.apply_fn(params, x, False)\n",
        "      loss = optax.softmax_cross_entropy_with_integer_labels(logits, y).mean()\n",
        "      print(loss)\n",
        "      return loss\n",
        "\n",
        "  loss, grads = jax.value_and_grad(loss_fn, has_aux=False)(state.params)\n",
        "  new_state = state.apply_gradients(grads=grads)\n",
        "  return loss, new_state"
      ],
      "metadata": {
        "id": "uyq-t1hPXtxp"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import time, math\n",
        "train_steps = 50\n",
        "data_loader = DataLoader(B=4, T=128)\n",
        "x, y = data_loader.next_batch()\n",
        "for step in range(train_steps):\n",
        "  t0 = time.time()\n",
        "  loss, train_state = train_step(train_state, x, y)\n",
        "  t1 = time.time()\n",
        "  dt = t1-t0\n",
        "\n",
        "  tokens_processed = data_loader.B * data_loader.T\n",
        "  tokens_per_sec = tokens_processed/dt\n",
        "\n",
        "  print(f\"step {step}/{train_steps} | loss : {loss:.4f} | dt {dt*1000 :.2f}ms | token/sec = {tokens_per_sec:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1K2hCMRXuIB",
        "outputId": "36cd3a4d-8b79-4932-a155-cec28e420b82"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded 338025 tokens in the datasets\n",
            " 1 epoch = 660 batches\n",
            "Traced<ShapedArray(float32[])>with<JVPTrace> with\n",
            "  primal = Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>\n",
            "  tangent = Traced<ShapedArray(float32[])>with<JaxprTrace> with\n",
            "    pval = (ShapedArray(float32[]), None)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7bf49468f1c0>, in_tracers=(Traced<ShapedArray(float32[4,128]):JaxprTrace>,), out_tracer_refs=[<weakref at 0x7bf4cd794590; to 'JaxprTracer' at 0x7bf4cd796d00>], out_avals=[ShapedArray(float32[])], primitive=pjit, params={'jaxpr': { lambda ; a:f32[4,128]. let\n",
            "    b:f32[] = reduce_sum[axes=(0, 1)] a\n",
            "    c:f32[] = div b 512.0\n",
            "  in (c,) }, 'in_shardings': (UnspecifiedValue,), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None,), 'out_layouts': (None,), 'donated_invars': (False,), 'ctx_mesh': None, 'name': '_mean', 'keep_unused': False, 'inline': True, 'compiler_options_kvs': ()}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x7bf4cc221060>, ctx=JaxprEqnContext(compute_type=None, threefry_partitionable=True, cur_abstract_mesh=AbstractMesh((), axis_types=()), xla_metadata=None))\n",
            "step 0/50 | loss : nan | dt 16940.20ms | token/sec = 30.224\n",
            "step 1/50 | loss : nan | dt 13.45ms | token/sec = 38075.951\n",
            "step 2/50 | loss : nan | dt 12.61ms | token/sec = 40602.062\n",
            "step 3/50 | loss : nan | dt 11.22ms | token/sec = 45613.501\n",
            "step 4/50 | loss : nan | dt 12.91ms | token/sec = 39646.340\n",
            "step 5/50 | loss : nan | dt 12.01ms | token/sec = 42646.033\n",
            "step 6/50 | loss : nan | dt 11.37ms | token/sec = 45022.509\n",
            "step 7/50 | loss : nan | dt 10.74ms | token/sec = 47690.065\n",
            "step 8/50 | loss : nan | dt 11.10ms | token/sec = 46108.076\n",
            "step 9/50 | loss : nan | dt 11.45ms | token/sec = 44729.924\n",
            "step 10/50 | loss : nan | dt 11.64ms | token/sec = 43999.501\n",
            "step 11/50 | loss : nan | dt 10.53ms | token/sec = 48641.728\n",
            "step 12/50 | loss : nan | dt 11.34ms | token/sec = 45139.859\n",
            "step 13/50 | loss : nan | dt 11.45ms | token/sec = 44716.884\n",
            "step 14/50 | loss : nan | dt 11.53ms | token/sec = 44397.934\n",
            "step 15/50 | loss : nan | dt 10.75ms | token/sec = 47641.398\n",
            "step 16/50 | loss : nan | dt 10.80ms | token/sec = 47405.820\n",
            "step 17/50 | loss : nan | dt 10.65ms | token/sec = 48058.267\n",
            "step 18/50 | loss : nan | dt 11.87ms | token/sec = 43127.358\n",
            "step 19/50 | loss : nan | dt 11.33ms | token/sec = 45192.105\n",
            "step 20/50 | loss : nan | dt 12.29ms | token/sec = 41655.034\n",
            "step 21/50 | loss : nan | dt 12.22ms | token/sec = 41902.938\n",
            "step 22/50 | loss : nan | dt 12.27ms | token/sec = 41735.179\n",
            "step 23/50 | loss : nan | dt 11.73ms | token/sec = 43666.680\n",
            "step 24/50 | loss : nan | dt 12.07ms | token/sec = 42433.679\n",
            "step 25/50 | loss : nan | dt 10.99ms | token/sec = 46571.037\n",
            "step 26/50 | loss : nan | dt 11.01ms | token/sec = 46504.475\n",
            "step 27/50 | loss : nan | dt 10.89ms | token/sec = 47020.728\n",
            "step 28/50 | loss : nan | dt 10.33ms | token/sec = 49546.262\n",
            "step 29/50 | loss : nan | dt 11.10ms | token/sec = 46111.046\n",
            "step 30/50 | loss : nan | dt 11.16ms | token/sec = 45897.191\n",
            "step 31/50 | loss : nan | dt 11.53ms | token/sec = 44404.360\n",
            "step 32/50 | loss : nan | dt 11.42ms | token/sec = 44824.222\n",
            "step 33/50 | loss : nan | dt 11.12ms | token/sec = 46055.667\n",
            "step 34/50 | loss : nan | dt 11.77ms | token/sec = 43495.983\n",
            "step 35/50 | loss : nan | dt 10.55ms | token/sec = 48553.746\n",
            "step 36/50 | loss : nan | dt 11.49ms | token/sec = 44577.649\n",
            "step 37/50 | loss : nan | dt 11.40ms | token/sec = 44896.380\n",
            "step 38/50 | loss : nan | dt 11.07ms | token/sec = 46234.147\n",
            "step 39/50 | loss : nan | dt 10.91ms | token/sec = 46929.276\n",
            "step 40/50 | loss : nan | dt 11.45ms | token/sec = 44713.160\n",
            "step 41/50 | loss : nan | dt 11.24ms | token/sec = 45533.228\n",
            "step 42/50 | loss : nan | dt 11.19ms | token/sec = 45761.244\n",
            "step 43/50 | loss : nan | dt 10.92ms | token/sec = 46881.124\n",
            "step 44/50 | loss : nan | dt 10.08ms | token/sec = 50787.145\n",
            "step 45/50 | loss : nan | dt 10.75ms | token/sec = 47646.683\n",
            "step 46/50 | loss : nan | dt 10.81ms | token/sec = 47379.672\n",
            "step 47/50 | loss : nan | dt 11.34ms | token/sec = 45142.706\n",
            "step 48/50 | loss : nan | dt 12.66ms | token/sec = 40436.923\n",
            "step 49/50 | loss : nan | dt 11.69ms | token/sec = 43788.664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader = DataLoader(B=4, T=128)\n",
        "x, y = data_loader.next_batch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGEFIU2AYyhK",
        "outputId": "23c97a84-ef2d-4a40-cd10-5271276fbf76"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded 338025 tokens in the datasets\n",
            " 1 epoch = 660 batches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXHGy96tbiuG",
        "outputId": "45757c74-2644-4150-a921-95bfd38c5124"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 5962 22307    25   198  8421   356  5120   597  2252    11  3285   502\n",
            "   2740    13   198   198  3237    25   198  5248   461    11  2740    13\n",
            "    198   198  5962 22307    25   198  1639   389   477 12939  2138   284\n",
            "   4656   621   284  1145   680    30   198   198  3237    25   198  4965\n",
            "   5634    13 12939    13   198   198  5962 22307    25   198  5962    11\n",
            "    345   760   327  1872   385  1526 28599   318  4039  4472   284   262\n",
            "    661    13   198   198  3237    25   198  1135   760   470    11   356\n",
            "    760   470    13   198   198  5962 22307    25   198  5756   514  1494\n",
            "    683    11   290   356  1183   423 11676   379   674   898  2756    13\n",
            "    198  3792   470   257 15593    30   198   198  3237    25   198  2949\n",
            "    517  3375   319   470    26  1309   340   307]\n",
            " [ 1760    25  1497    11  1497     0   198   198 12211 22307    25   198\n",
            "   3198  1573    11   922  4290    13   198   198  5962 22307    25   198\n",
            "   1135   389 17830  3595  4290    11   262  1458  1173  1547   922    13\n",
            "    198  2061  4934   969  5036   896   319   561 26958   514    25   611\n",
            "    484   198 19188  7800   514   475   262 48713   414    11   981   340\n",
            "    547   198  1929  4316   462    11   356  1244  4724   484 22598   514\n",
            "  31533   306    26   198  4360   484   892   356   389  1165 13674    25\n",
            "    262 10904  1108   326   198  2001 42267   514    11   262  2134   286\n",
            "    674 24672    11   318   355   281   198 24807   284  1948   786   511\n",
            "  20038    26   674   198    82 13712   590   318   257  4461   284   606\n",
            "   3914   514 15827   428   351   198   454   279]\n",
            " [ 7938    11   304   260   356  1716   374  1124    25   329   262 11858\n",
            "    760   314   198 47350   428   287 16460   329  8509    11   407   287\n",
            "  24613   329 15827    13   198   198 12211 22307    25   198 17353   345\n",
            "   5120  2592  1028   327  1872   385  1526 28599    30   198   198  3237\n",
            "     25   198 39276   683   717    25   339   338   257   845  3290   284\n",
            "    262  2219  6017    13   198   198 12211 22307    25   198 19626   345\n",
            "    644  2594   339   468  1760   329   465  1499    30   198   198  5962\n",
            "  22307    25   198 16371   880    26   290   714   307  2695   284  1577\n",
            "    683   922   198 13116  6285    11   475   326   339 13831  2241   351\n",
            "    852  6613    13   198   198 12211 22307    25   198    45   323    11\n",
            "    475  2740   407 17412   306    13   198   198]\n",
            " [ 5962 22307    25   198    40   910 12722   345    11   644   339 22027\n",
            "   1760 20524    11   339   750   198   270   284   326   886    25   996\n",
            "   2705    12  5936   979  5864  1450   460   307   198 11299   284   910\n",
            "    340   373   329   465  1499   339   750   340   284   198 29688   465\n",
            "   2802   290   284   307 11476  6613    26   543   339   198   271    11\n",
            "    772 10597   262 20334   286   465 14675    13   198   198 12211 22307\n",
            "     25   198  2061   339  2314  1037   287   465  3450    11   345  1848\n",
            "    257   198 28281   287   683    13   921  1276   287   645   835   910\n",
            "    339   318 25746    83   516    13   198   198  5962 22307    25   198\n",
            "   1532   314  1276   407    11   314   761   407   307 39497   286 14227\n",
            "     26   198   258 22027 31025    11   351 18201]]\n",
            "[[22307    25   198  8421   356  5120   597  2252    11  3285   502  2740\n",
            "     13   198   198  3237    25   198  5248   461    11  2740    13   198\n",
            "    198  5962 22307    25   198  1639   389   477 12939  2138   284  4656\n",
            "    621   284  1145   680    30   198   198  3237    25   198  4965  5634\n",
            "     13 12939    13   198   198  5962 22307    25   198  5962    11   345\n",
            "    760   327  1872   385  1526 28599   318  4039  4472   284   262   661\n",
            "     13   198   198  3237    25   198  1135   760   470    11   356   760\n",
            "    470    13   198   198  5962 22307    25   198  5756   514  1494   683\n",
            "     11   290   356  1183   423 11676   379   674   898  2756    13   198\n",
            "   3792   470   257 15593    30   198   198  3237    25   198  2949   517\n",
            "   3375   319   470    26  1309   340   307  1760]\n",
            " [   25  1497    11  1497     0   198   198 12211 22307    25   198  3198\n",
            "   1573    11   922  4290    13   198   198  5962 22307    25   198  1135\n",
            "    389 17830  3595  4290    11   262  1458  1173  1547   922    13   198\n",
            "   2061  4934   969  5036   896   319   561 26958   514    25   611   484\n",
            "    198 19188  7800   514   475   262 48713   414    11   981   340   547\n",
            "    198  1929  4316   462    11   356  1244  4724   484 22598   514 31533\n",
            "    306    26   198  4360   484   892   356   389  1165 13674    25   262\n",
            "  10904  1108   326   198  2001 42267   514    11   262  2134   286   674\n",
            "  24672    11   318   355   281   198 24807   284  1948   786   511 20038\n",
            "     26   674   198    82 13712   590   318   257  4461   284   606  3914\n",
            "    514 15827   428   351   198   454   279  7938]\n",
            " [   11   304   260   356  1716   374  1124    25   329   262 11858   760\n",
            "    314   198 47350   428   287 16460   329  8509    11   407   287 24613\n",
            "    329 15827    13   198   198 12211 22307    25   198 17353   345  5120\n",
            "   2592  1028   327  1872   385  1526 28599    30   198   198  3237    25\n",
            "    198 39276   683   717    25   339   338   257   845  3290   284   262\n",
            "   2219  6017    13   198   198 12211 22307    25   198 19626   345   644\n",
            "   2594   339   468  1760   329   465  1499    30   198   198  5962 22307\n",
            "     25   198 16371   880    26   290   714   307  2695   284  1577   683\n",
            "    922   198 13116  6285    11   475   326   339 13831  2241   351   852\n",
            "   6613    13   198   198 12211 22307    25   198    45   323    11   475\n",
            "   2740   407 17412   306    13   198   198  5962]\n",
            " [22307    25   198    40   910 12722   345    11   644   339 22027  1760\n",
            "  20524    11   339   750   198   270   284   326   886    25   996  2705\n",
            "     12  5936   979  5864  1450   460   307   198 11299   284   910   340\n",
            "    373   329   465  1499   339   750   340   284   198 29688   465  2802\n",
            "    290   284   307 11476  6613    26   543   339   198   271    11   772\n",
            "  10597   262 20334   286   465 14675    13   198   198 12211 22307    25\n",
            "    198  2061   339  2314  1037   287   465  3450    11   345  1848   257\n",
            "    198 28281   287   683    13   921  1276   287   645   835   910   339\n",
            "    318 25746    83   516    13   198   198  5962 22307    25   198  1532\n",
            "    314  1276   407    11   314   761   407   307 39497   286 14227    26\n",
            "    198   258 22027 31025    11   351 18201    11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JbkTv31ybk3u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}