{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4MQc64FbeZtt"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ModelConfig:\n",
        "  vocab_size : int = 50257\n",
        "  n_head : int = 12\n",
        "  n_embed : int = 768\n",
        "  n_layer : int = 12\n",
        "  block_size : int = 1024\n",
        "  dropout_rate : float = 0.1\n",
        "  gradient_accumulation_steps : int = 16\n",
        "  dtype : jnp.dtype = jnp.bfloat16\n"
      ],
      "metadata": {
        "id": "13lCGJksfDIy"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp"
      ],
      "metadata": {
        "id": "vieAua-CXYvF"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flax import linen as nn\n",
        "import jax.numpy as jnp\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "  config:ModelConfig\n",
        "\n",
        "  @nn.compact\n",
        "\n",
        "  def __call__(self, x, deterministic = True):\n",
        "    assert len(x.shape)==3\n",
        "    b, l, d = x.shape\n",
        "\n",
        "    q = nn.Dense(self.config.n_embed)(x)\n",
        "    k = nn.Dense(self.config.n_embed)(x)\n",
        "    v = nn.Dense(self.config.n_embed)(x)\n",
        "\n",
        "    q = jnp.reshape(q, (b, l, d//self.config.n_head, self.config.n_head)).astype(jnp.float32)\n",
        "    k = jnp.reshape(k, (b, l, d//self.config.n_head, self.config.n_head)).astype(jnp.float32)\n",
        "    v = jnp.reshape(v, (b, l, d//self.config.n_head, self.config.n_head))\n",
        "\n",
        "    norm = jnp.sqrt(list(jnp.shape(k))[-1])\n",
        "\n",
        "    attn = jnp.matmul(q, jnp.transpose(k, (0, 1, 3, 2))) / norm\n",
        "    mask = jnp.tril(attn)\n",
        "\n",
        "    attn = jnp.where(mask[:, :, :l, :l], attn, float(\"-inf\")).astype(jnp.float32)\n",
        "    probs = jax.nn.softmax(attn, axis=-1).astype(self.config.dtype)\n",
        "    y = jnp.matmul(probs, v)\n",
        "    y = jnp.reshape(y, (b, l, d))\n",
        "    y = nn.Dense(self.config.n_embed)(y).astype(self.config.dtype)\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YPLJw7g3fzzQ"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "  config : ModelConfig\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, deterministic = True):\n",
        "    x=nn.Dense(self.config.n_embed*4)(x)\n",
        "    x=nn.gelu(x)\n",
        "    x=nn.Dropout(rate=self.config.dropout_rate)(x, deterministic=True)\n",
        "    x=nn.Dense(self.config.n_embed)(x)\n",
        "    x=nn.Dropout(rate=self.config.dropout_rate)(x, deterministic=True)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "  config : ModelConfig\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    x = nn.LayerNorm()(x)\n",
        "    x = x + CausalSelfAttention(self.config)(x)\n",
        "    x = nn.LayerNorm()(x)\n",
        "    x = x + MLP(self.config)(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "kIxHxhzdnLQF"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "  config : ModelConfig\n",
        "\n",
        "  @nn.compact\n",
        "\n",
        "  def __call__(self, x, deterministic=True):\n",
        "    B, T = x.shape\n",
        "    assert T <= self.config.block_size\n",
        "\n",
        "    pos = jnp.arange(0, T)[None]\n",
        "    pos_emb = nn.Embed(self.config.block_size, self.config.n_embed)(pos)\n",
        "    wte = nn.Embed(self.config.vocab_size, self.config.n_embed)\n",
        "    tok_emb = wte(x)\n",
        "    x = tok_emb + pos_emb\n",
        "\n",
        "    for _ in range(self.config.n_layer):\n",
        "      x = Block(self.config)(x)\n",
        "    x = nn.LayerNorm()(x)\n",
        "    # logits = nn.Dense(self.config.vocab_size)(x)\n",
        "    logits = wte.attend(x)\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n",
        "  def init(self, rng):\n",
        "    tokens = jnp.zeros((1, self.config.block_size), dtype=jnp.uint16)\n",
        "    params = jax.jit(super().init, static_argnums=(2,))(rng, tokens, True)\n",
        "    return params\n",
        "\n"
      ],
      "metadata": {
        "id": "Dbfc51LYnLNv"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_params(params):\n",
        "  p=jax.tree_util.tree_map(lambda a: a.size if isinstance(a, jnp.ndarray) else 0, params)\n",
        "  return jax.tree_util.tree_reduce(lambda a, b : a+b, p)"
      ],
      "metadata": {
        "id": "AURmPVe8nLLF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = ModelConfig()\n",
        "key = jax.random.PRNGKey(0)\n",
        "# model = GPT(config)\n",
        "# params = model.init(key)\n",
        "# count_params(params)"
      ],
      "metadata": {
        "id": "xyho-hDRnLIE"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "8yO-7I3zv-Jb",
        "outputId": "54415877-68cd-44a5-ca78-5b141ab15c23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT(\n",
            "    # attributes\n",
            "    config = ModelConfig(vocab_size=50257, n_head=12, n_embed=768, n_layer=12, block_size=1024, dropout_rate=0.1)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4OxtyAWoiBSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoModelForCausalLM\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "# print(model)\n"
      ],
      "metadata": {
        "id": "bS-gt9ADs32_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyoYTh7Ns30T",
        "outputId": "c7d91144-09da-47e0-fc24-5e6d6ded733c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'params': {'Dense_0': {'kernel': Array([[-0.95366824,  0.43563786, -0.7954482 , -0.49190977],\n",
              "          [ 0.5308177 ,  0.74109775,  0.6027838 , -0.02684463],\n",
              "          [ 0.24410059,  0.44881055, -1.050442  ,  0.4932145 ]],      dtype=float32),\n",
              "   'bias': Array([0., 0., 0., 0.], dtype=float32)},\n",
              "  'Dense_1': {'kernel': Array([[ 0.00166371,  0.16012576],\n",
              "          [ 0.09040862, -0.42028674],\n",
              "          [ 0.32189375,  0.43688348],\n",
              "          [-0.5580085 , -0.36031362]], dtype=float32),\n",
              "   'bias': Array([0., 0.], dtype=float32)}}}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader:\n",
        "  def __init__(self, B, T):\n",
        "    self.current_position = 0\n",
        "    self.B = B\n",
        "    self.T = T\n",
        "\n",
        "    with open(\"/content/input.txt\",\"r\") as f:\n",
        "      text = f.read()\n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "    self.tokens = jnp.array(enc.encode(text))\n",
        "    print(f\"loaded {len(self.tokens)} tokens in the datasets\" )\n",
        "    print(f\" 1 epoch = {len(self.tokens)//(B*T)} batches\")\n",
        "\n",
        "  def next_batch(self):\n",
        "    B,T = self.B, self.T\n",
        "    buf = self.tokens[self.current_position:self.current_position+B*T+1]\n",
        "    x,y = jnp.reshape(buf[:-1],(B,T)), jnp.reshape(buf[1:],(B,T))\n",
        "    self.current_position += B*T\n",
        "    if self.current_position + B*T+1 > len(self.tokens):\n",
        "      self.current_position = 0\n",
        "    return x,y"
      ],
      "metadata": {
        "id": "g1M-SMxPtDzy"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "from flax import linen as nn\n",
        "from flax.training.train_state import TrainState  # <- THIS is the TrainState you need\n",
        "from flax.core import FrozenDict\n",
        "from typing import Tuple\n",
        "\n",
        "\n",
        "def init_train_state(key, config) -> TrainState:\n",
        "  model = nn.remat(\n",
        "        GPT, policy=jax.checkpoint_policies.checkpoint_dots_with_no_batch_dims\n",
        "        )(config)\n",
        "\n",
        "  params = model.init(key)\n",
        "  learning_rate = optax.warmup_cosine_decay_schedule(\n",
        "      init_value=0.0,\n",
        "      peak_value=2.5e-4,\n",
        "      warmup_steps= 2000,\n",
        "      decay_steps= 150000,\n",
        "      end_value = 1e-5,\n",
        ")\n",
        "  optimizer = optax.chain(\n",
        "      optax.clip_by_global_norm(1.0),\n",
        "      optax.adamw(learning_rate, b1=0.9, b2=0.95,  weight_decay=1e-2)\n",
        "\n",
        "  if config.gradient_accumulation_steps>1:\n",
        "    optimizer = optax.MultiSteps(\n",
        "          optimizer, every_k_schedule=config.gradient_accumulation_steps\n",
        "    )\n",
        ")\n",
        "  train_state = TrainState.create(\n",
        "        apply_fn=model.apply,\n",
        "        params=params,\n",
        "        tx=optimizer)\n",
        "  return train_state\n",
        "\n",
        "@jax.jit\n",
        "def train_step(state: TrainState, x: jnp.ndarray, y: jnp.ndarray) -> Tuple[jnp.ndarray, TrainState]:\n",
        "\n",
        "  def loss_fn(params: FrozenDict) -> jnp.ndarray:\n",
        "\n",
        "      logits = state.apply_fn(params, x, False)\n",
        "      loss = optax.softmax_cross_entropy_with_integer_labels(logits, y).mean()\n",
        "      print(loss)\n",
        "      return loss\n",
        "\n",
        "  loss, grads = jax.value_and_grad(loss_fn, has_aux=False)(state.params)\n",
        "  new_state = state.apply_gradients(grads=grads)\n",
        "  return loss, new_state"
      ],
      "metadata": {
        "id": "uyq-t1hPXtxp"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import time, math\n",
        "rng = jax.random.PRNGKey(0)\n",
        "train_state = init_train_state(rng, ModelConfig())\n",
        "train_steps = 50\n",
        "data_loader = DataLoader(B=4, T=128)\n",
        "x, y = data_loader.next_batch()\n",
        "for step in range(train_steps):\n",
        "  t0 = time.time()\n",
        "  loss, train_state = train_step(train_state, x, y)\n",
        "  t1 = time.time()\n",
        "  dt = t1-t0\n",
        "\n",
        "  tokens_processed = data_loader.B * data_loader.T\n",
        "  tokens_per_sec = tokens_processed/dt\n",
        "\n",
        "  print(f\"step {step}/{train_steps} | loss : {loss:.4f} | dt {dt*1000 :.2f}ms | token/sec = {tokens_per_sec:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1K2hCMRXuIB",
        "outputId": "0fafa678-f261-4738-d31c-e97df8c79111"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded 338025 tokens in the datasets\n",
            " 1 epoch = 660 batches\n",
            "Traced<ShapedArray(float32[])>with<JVPTrace> with\n",
            "  primal = Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>\n",
            "  tangent = Traced<ShapedArray(float32[])>with<JaxprTrace> with\n",
            "    pval = (ShapedArray(float32[]), None)\n",
            "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7bf494721970>, in_tracers=(Traced<ShapedArray(float32[4,128]):JaxprTrace>,), out_tracer_refs=[<weakref at 0x7bf4cd71e9d0; to 'JaxprTracer' at 0x7bf4cd71d630>], out_avals=[ShapedArray(float32[])], primitive=pjit, params={'jaxpr': { lambda ; a:f32[4,128]. let\n",
            "    b:f32[] = reduce_sum[axes=(0, 1)] a\n",
            "    c:f32[] = div b 512.0\n",
            "  in (c,) }, 'in_shardings': (UnspecifiedValue,), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None,), 'out_layouts': (None,), 'donated_invars': (False,), 'ctx_mesh': None, 'name': '_mean', 'keep_unused': False, 'inline': True, 'compiler_options_kvs': ()}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x7bf520247160>, ctx=JaxprEqnContext(compute_type=None, threefry_partitionable=True, cur_abstract_mesh=AbstractMesh((), axis_types=()), xla_metadata=None))\n",
            "step 0/50 | loss : 11.4260 | dt 23497.34ms | token/sec = 21.790\n",
            "step 1/50 | loss : 10.2885 | dt 853.95ms | token/sec = 599.570\n",
            "step 2/50 | loss : 9.8338 | dt 13.64ms | token/sec = 37548.015\n",
            "step 3/50 | loss : 8.6391 | dt 12.09ms | token/sec = 42354.173\n",
            "step 4/50 | loss : 7.7738 | dt 12.21ms | token/sec = 41925.025\n",
            "step 5/50 | loss : 7.2781 | dt 11.29ms | token/sec = 45358.193\n",
            "step 6/50 | loss : 8.0010 | dt 13.44ms | token/sec = 38100.271\n",
            "step 7/50 | loss : 7.2962 | dt 12.29ms | token/sec = 41661.499\n",
            "step 8/50 | loss : 7.0896 | dt 16.75ms | token/sec = 30567.422\n",
            "step 9/50 | loss : 6.7432 | dt 12.03ms | token/sec = 42572.481\n",
            "step 10/50 | loss : 6.2707 | dt 12.85ms | token/sec = 39842.736\n",
            "step 11/50 | loss : 5.8536 | dt 11.69ms | token/sec = 43797.594\n",
            "step 12/50 | loss : 5.7919 | dt 11.67ms | token/sec = 43868.275\n",
            "step 13/50 | loss : 6.2623 | dt 11.22ms | token/sec = 45646.466\n",
            "step 14/50 | loss : 6.2166 | dt 12.52ms | token/sec = 40879.533\n",
            "step 15/50 | loss : 5.4916 | dt 11.87ms | token/sec = 43148.154\n",
            "step 16/50 | loss : 5.5739 | dt 12.13ms | token/sec = 42222.600\n",
            "step 17/50 | loss : 5.5206 | dt 11.57ms | token/sec = 44265.236\n",
            "step 18/50 | loss : 5.3580 | dt 12.21ms | token/sec = 41922.570\n",
            "step 19/50 | loss : 5.1480 | dt 11.60ms | token/sec = 44146.030\n",
            "step 20/50 | loss : 4.9134 | dt 11.75ms | token/sec = 43557.739\n",
            "step 21/50 | loss : 4.7196 | dt 11.93ms | token/sec = 42911.053\n",
            "step 22/50 | loss : 4.6479 | dt 11.60ms | token/sec = 44141.493\n",
            "step 23/50 | loss : 4.5951 | dt 11.62ms | token/sec = 44054.561\n",
            "step 24/50 | loss : 4.5152 | dt 11.29ms | token/sec = 45335.212\n",
            "step 25/50 | loss : 4.4527 | dt 10.68ms | token/sec = 47944.535\n",
            "step 26/50 | loss : 4.3618 | dt 11.41ms | token/sec = 44878.553\n",
            "step 27/50 | loss : 4.5948 | dt 10.67ms | token/sec = 47962.738\n",
            "step 28/50 | loss : 4.4494 | dt 11.60ms | token/sec = 44146.938\n",
            "step 29/50 | loss : 4.2277 | dt 11.87ms | token/sec = 43129.090\n",
            "step 30/50 | loss : 4.2215 | dt 12.00ms | token/sec = 42665.521\n",
            "step 31/50 | loss : 4.1299 | dt 11.81ms | token/sec = 43361.608\n",
            "step 32/50 | loss : 4.1177 | dt 11.40ms | token/sec = 44921.737\n",
            "step 33/50 | loss : 4.0667 | dt 11.48ms | token/sec = 44600.794\n",
            "step 34/50 | loss : 4.0660 | dt 11.73ms | token/sec = 43655.140\n",
            "step 35/50 | loss : 3.9847 | dt 11.74ms | token/sec = 43614.356\n",
            "step 36/50 | loss : 4.0148 | dt 12.26ms | token/sec = 41759.526\n",
            "step 37/50 | loss : 4.0242 | dt 11.68ms | token/sec = 43847.673\n",
            "step 38/50 | loss : 4.0579 | dt 11.90ms | token/sec = 43029.708\n",
            "step 39/50 | loss : 3.9646 | dt 12.14ms | token/sec = 42168.708\n",
            "step 40/50 | loss : 3.9539 | dt 11.99ms | token/sec = 42696.907\n",
            "step 41/50 | loss : 3.9425 | dt 10.99ms | token/sec = 46583.159\n",
            "step 42/50 | loss : 3.9866 | dt 11.77ms | token/sec = 43506.557\n",
            "step 43/50 | loss : 3.8984 | dt 11.51ms | token/sec = 44488.992\n",
            "step 44/50 | loss : 3.8620 | dt 12.21ms | token/sec = 41941.402\n",
            "step 45/50 | loss : 3.7769 | dt 11.48ms | token/sec = 44618.401\n",
            "step 46/50 | loss : 3.7745 | dt 11.78ms | token/sec = 43474.849\n",
            "step 47/50 | loss : 3.7237 | dt 10.99ms | token/sec = 46607.424\n",
            "step 48/50 | loss : 3.6943 | dt 11.41ms | token/sec = 44886.057\n",
            "step 49/50 | loss : 3.6199 | dt 10.71ms | token/sec = 47806.849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader = DataLoader(B=4, T=128)\n",
        "x, y = data_loader.next_batch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGEFIU2AYyhK",
        "outputId": "23c97a84-ef2d-4a40-cd10-5271276fbf76"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded 338025 tokens in the datasets\n",
            " 1 epoch = 660 batches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for step in range(train_steps):\n",
        "  t0 = time.time()\n",
        "  for _ in range(config.gradient_accumulation_steps):\n",
        "    x,y = data_loader.next_batch()\n",
        "    loss, train_state = train_step(train_state, x, y)\n",
        "  t1 = time.time()\n",
        "  dt = t1 - t0\n",
        "  tokens_processed = data_loader.B * data_loader.T * config.gradient_accumulation_steps\n",
        "  tokens_per_sec = tokens_processed / dt\n",
        "  print(f\"step {step}/{train_steps} | loss: {loss:4f} |  dt: {dt*1000:.2f}ms | token/sec = {tokens_per_sec:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXHGy96tbiuG",
        "outputId": "b6e8d56e-98ee-4048-a631-b4cb83c5133e"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0/50 | loss: 6.206775 |  dt: 454.12ms | token/sec = 18039.17\n",
            "step 1/50 | loss: 6.237356 |  dt: 695.14ms | token/sec = 11784.76\n",
            "step 2/50 | loss: 5.584978 |  dt: 460.25ms | token/sec = 17799.06\n",
            "step 3/50 | loss: 6.498955 |  dt: 472.78ms | token/sec = 17327.16\n",
            "step 4/50 | loss: 5.995944 |  dt: 472.78ms | token/sec = 17327.21\n",
            "step 5/50 | loss: 6.367478 |  dt: 464.63ms | token/sec = 17631.38\n",
            "step 6/50 | loss: 5.633333 |  dt: 464.19ms | token/sec = 17647.76\n",
            "step 7/50 | loss: 6.112411 |  dt: 470.71ms | token/sec = 17403.58\n",
            "step 8/50 | loss: 6.181900 |  dt: 470.95ms | token/sec = 17394.65\n",
            "step 9/50 | loss: 6.271253 |  dt: 465.72ms | token/sec = 17590.06\n",
            "step 10/50 | loss: 5.743571 |  dt: 469.21ms | token/sec = 17459.28\n",
            "step 11/50 | loss: 5.862834 |  dt: 472.38ms | token/sec = 17341.98\n",
            "step 12/50 | loss: 6.046236 |  dt: 470.13ms | token/sec = 17425.07\n",
            "step 13/50 | loss: 6.785887 |  dt: 466.11ms | token/sec = 17575.13\n",
            "step 14/50 | loss: 5.924488 |  dt: 469.74ms | token/sec = 17439.34\n",
            "step 15/50 | loss: 6.489791 |  dt: 471.73ms | token/sec = 17365.74\n",
            "step 16/50 | loss: 6.156429 |  dt: 468.84ms | token/sec = 17472.89\n",
            "step 17/50 | loss: 6.662457 |  dt: 470.54ms | token/sec = 17409.74\n",
            "step 18/50 | loss: 6.622251 |  dt: 469.89ms | token/sec = 17433.70\n",
            "step 19/50 | loss: 5.920865 |  dt: 472.87ms | token/sec = 17324.11\n",
            "step 20/50 | loss: 6.243437 |  dt: 469.17ms | token/sec = 17460.74\n",
            "step 21/50 | loss: 6.037891 |  dt: 472.53ms | token/sec = 17336.35\n",
            "step 22/50 | loss: 6.529212 |  dt: 471.39ms | token/sec = 17378.44\n",
            "step 23/50 | loss: 6.450150 |  dt: 472.48ms | token/sec = 17338.30\n",
            "step 24/50 | loss: 6.140039 |  dt: 471.84ms | token/sec = 17361.70\n",
            "step 25/50 | loss: 5.756162 |  dt: 471.61ms | token/sec = 17370.22\n",
            "step 26/50 | loss: 5.329160 |  dt: 471.51ms | token/sec = 17373.93\n",
            "step 27/50 | loss: 6.708566 |  dt: 472.79ms | token/sec = 17326.91\n",
            "step 28/50 | loss: 6.615991 |  dt: 469.49ms | token/sec = 17448.83\n",
            "step 29/50 | loss: 5.844440 |  dt: 471.08ms | token/sec = 17389.75\n",
            "step 30/50 | loss: 5.471779 |  dt: 472.18ms | token/sec = 17349.25\n",
            "step 31/50 | loss: 6.486062 |  dt: 472.13ms | token/sec = 17351.28\n",
            "step 32/50 | loss: 6.504060 |  dt: 471.17ms | token/sec = 17386.38\n",
            "step 33/50 | loss: 6.570214 |  dt: 470.43ms | token/sec = 17413.97\n",
            "step 34/50 | loss: 6.332332 |  dt: 470.73ms | token/sec = 17402.78\n",
            "step 35/50 | loss: 6.507184 |  dt: 469.14ms | token/sec = 17461.62\n",
            "step 36/50 | loss: 6.217346 |  dt: 468.46ms | token/sec = 17487.01\n",
            "step 37/50 | loss: 6.411785 |  dt: 471.01ms | token/sec = 17392.49\n",
            "step 38/50 | loss: 6.638424 |  dt: 470.61ms | token/sec = 17407.35\n",
            "step 39/50 | loss: 6.168045 |  dt: 469.97ms | token/sec = 17430.86\n",
            "step 40/50 | loss: 6.603686 |  dt: 468.46ms | token/sec = 17487.04\n",
            "step 41/50 | loss: 6.474263 |  dt: 466.23ms | token/sec = 17570.78\n",
            "step 42/50 | loss: 6.639009 |  dt: 468.39ms | token/sec = 17489.76\n",
            "step 43/50 | loss: 6.536925 |  dt: 467.83ms | token/sec = 17510.52\n",
            "step 44/50 | loss: 6.785682 |  dt: 466.31ms | token/sec = 17567.53\n",
            "step 45/50 | loss: 5.946084 |  dt: 466.78ms | token/sec = 17550.03\n",
            "step 46/50 | loss: 6.296563 |  dt: 468.69ms | token/sec = 17478.67\n",
            "step 47/50 | loss: 6.434015 |  dt: 469.15ms | token/sec = 17461.40\n",
            "step 48/50 | loss: 5.962573 |  dt: 469.81ms | token/sec = 17436.83\n",
            "step 49/50 | loss: 6.704974 |  dt: 468.82ms | token/sec = 17473.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JbkTv31ybk3u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}